<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>mext-bibliography</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__left">
    <div class="stackedit__toc">

      <ul>
        <li><a href="#bibliography">Bibliography</a>
          <ul>
            <li><a href="#ai-regulation-and-cultural-bias">AI Regulation and Cultural Bias</a></li>
          </ul>
        </li>
      </ul>

    </div>
  </div>
  <div class="stackedit__right">
    <div class="stackedit__html">
      <h1 id="bibliography">Bibliography</h1>
      <h2 id="ai-regulation-and-cultural-bias">AI Regulation and Cultural Bias</h2>
      <h3 id="western-bias-in-ai">Western Bias in AI</h3>
      <ul>
        <li><strong>Zhu, L., Mou, W., Lai, Y. et al.</strong> (2024). “Language and cultural bias in AI: comparing the
          performance of large language models developed in different countries on Traditional Chinese Medicine
          highlights the need for localized models,” <em><strong>Journal of Translational Medicine</strong></em>, 22,
          319. <a href="https://doi.org/10.1186/s12967-024-03598-6">DOI: 10.1186/s12967-024-03598-6</a>.</li>
        <li><strong>Malik et al.</strong> (2022). “Socially Aware Bias Measurements for Hindi Language Representations,”
          <em><strong>NAACL 2022</strong></em>.</li>
        <li><strong>Bhatt et al.</strong> (2022). “Re-contextualizing Fairness in NLP: The Case of India,”
          <em><strong>AACL-IJCNLP 2022</strong></em>.</li>
        <li><strong>Peters, Uwe, &amp; Carman, Mary.</strong> “Cultural Bias in Explainable AI Research: A Systematic
          Analysis.”</li>
      </ul>
      <h3 id="cross-cultural-studies-in-psychology-and-ai">Cross-Cultural Studies in Psychology and AI</h3>
      <ul>
        <li><strong>Masuda, T., &amp; Nisbett, R. E.</strong> (2001). “Attending holistically versus analytically:
          comparing the context sensitivity of Japanese and Americans,” <em><strong>Journal of Personality and Social
              Psychology</strong></em>, 81(5), 922-934.</li>
        <li><strong>Yiend J, André J, Smith L, Chen LH, Toulopoulou T, et al.</strong> (2019). “Biased cognition in East
          Asian and Western cultures,” <em><strong>PLOS ONE</strong></em>, 14(10): e0223358. <a
            href="https://doi.org/10.1371/journal.pone.0223358">DOI: 10.1371/journal.pone.0223358</a>.</li>
      </ul>
      <h3 id="ai-regulation">AI Regulation</h3>
      <ul>
        <li><strong>Japan’s Approach to AI Regulation and Its Impact on 2023 G7 Presidency</strong>. (2023). <a
            href="https://www.csis.org/analysis/japans-approach-ai-regulation-and-its-impact-2023-g7-presidency">CSIS</a>.
        </li>
      </ul>
      <h3 id="human-in-the-loop-and-ai-fairness">Human-in-the-loop and AI Fairness</h3>
      <ul>
        <li><strong>Nakao, Yuri, Stumpf, Simone, Ahmed, Subeida, Naseer, Aisha, &amp; Strappelli, Lorenzo.</strong>
          (2022). “Toward Involving End-users in Interactive Human-in-the-loop AI Fairness,” ***ACM Trans. Interact.
          Intell. Syst.***, 12(3), Article 18. <a href="https://doi.org/10.1145/3514258">DOI: 10.1145/3514258</a>.</li>
      </ul>
      <h3 id="human-ai-collaboration">Human-AI Collaboration</h3>
      <ul>
        <li><strong>De-Arteaga, M., Gao, R., Saar-Tsechansky, M., Han, L., Lee, MK, Sun, W., &amp; Lease, M.</strong>
          (2022). “Learning Complementary Policies for Human-AI Teams,” <em><strong>CIST’22</strong></em>. Best Student
          Paper Award.</li>
        <li><strong>De-Arteaga, M., Schoeffer, J., &amp; Kuehl, N.</strong> (2024). “Explanations, Fairness, and
          Appropriate Reliance in Human-AI Decision-Making,” ***Proceedings of ACM CHI Conference on Human Factors in
          Computing Systems (CHI)***, 2024. Honorable Mention Award (top 5%).</li>
      </ul>
      <h3 id="algorithmic-fairness-and-bias">Algorithmic Fairness and Bias</h3>
      <ul>
        <li><strong>De-Arteaga, M., Neumann, T., Lee, S., Fazelpour, S., &amp; Lease, M.</strong> “Diverse, but
          Divisive: LLMs Can Exaggerate Gender Differences in Opinion Related to Harms of Misinformation.”</li>
        <li><strong>De-Arteaga, M., Jeanselme, V., Dubrawski, A., &amp; Chouldechova, A.</strong> (2021). “Leveraging
          Expert Consistency to Improve Algorithmic Decision Support,” <em><strong>WITS’21</strong></em>. Best Paper
          Runner-Up Award.</li>
        <li><strong>Cheng, Myra, Mackey, Lester, Kalai, Adam, &amp; De-Arteaga, M.</strong> (2023). “Social Norm Bias:
          Residual Harms of Fairness-Aware Algorithms,” <em><strong>Data Mining &amp; Knowledge Discovery</strong></em>.
        </li>
        <li><strong>Feuerriegel, Stefan, Saar-Tsechansky, Maytal, &amp; De-Arteaga, M.</strong> (2022). “Algorithmic
          Fairness in Business Analytics: Directions for Research and Practice,” <em><strong>Production &amp; Operations
              Management</strong></em>.</li>
        <li><strong>De-Arteaga, M., Fogliato, R., &amp; Chouldechova, A.</strong> “A case for humans-in-the-loop:
          decisions in the presence of misestimated algorithmic scores.” Extended version of ***CHI</li>
      </ul>
      <hr>
      <h3 id="mext-scholarship-research-plan---juan-pablo-gutiérrez-restrepo">MEXT Scholarship Research Plan - Juan
        Pablo Gutiérrez Restrepo</h3>

    </div>
  </div>
</body>

</html>